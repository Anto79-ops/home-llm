{
  "config": {
    "step": {
      "user": {
        "title": "Select Backend",
        "description": "Select the backend for running the model. Either Llama.cpp (locally) or text-generation-webui (remote).",
        "data": {
          "use_local_backend": "Use Llama.cpp",
          "download_model_from_hf": "Download model from HuggingFace"
        }
      },
      "local_model": {
        "title": "Configure llama.cpp",
        "description": "Please configure llama.cpp for the model",
        "data": {
          "huggingface_model": "HuggingFace Model",
          "downloaded_model_file": "Local file name",
          "downloaded_model_quantization": "Downloaded model quantization"
        }
      },
      "remote_model": {
        "title": "Configure connection to remote API",
        "description": "Provide the connection details for an instance of text-generation-webui that is hosting the model.",
        "data": {
          "huggingface_model": "Model Name",
          "host": "API Hostname",
          "port": "API Port"
        }
      }
    },
    "error": {
      "unknown": "[%key:common::config_flow::error::unknown%]",
      "download_failed": "The download failed to complete!",
      "missing_model_file": "The provided file does not exist.",
      "missing_model_api": "The selected model is not provided by this API.",
      "failed_to_connect": "Failed to connect to the remote API. See the logs for more details.",
      "other_existing_local": "Another model is already loaded locally. Please unload it or configure a remote model."
    },
    "progress": {
      "download": "Please wait while the model is being downloaded from HuggingFace. This can take a few minutes."
    }
  },
  "options": {
    "step": {
      "init": {
        "data": {
          "prompt": "Prompt Template",
          "max_new_tokens": "Maximum tokens to return in response",
          "temperature": "Temperature",
          "top_k": "Top K",
          "top_p": "Top P"
        }
      }
    }
  }
}
